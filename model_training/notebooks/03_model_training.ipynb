{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfraOwl Model Training\n",
    "\n",
    "This notebook handles the complete model training pipeline for InfraOwl infrastructure detection.\n",
    "\n",
    "## Training Pipeline\n",
    "- Model architecture setup (EfficientNet-Lite/MobileNet)\n",
    "- Transfer learning configuration\n",
    "- Training with callbacks and monitoring\n",
    "- Model evaluation and visualization\n",
    "- TensorFlow Lite conversion for mobile deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Import our training modules\n",
    "from train_model import InfraOwlTrainer\n",
    "from convert_to_tflite import TFLiteConverter\n",
    "\n",
    "print(\"ğŸ¯ InfraOwl Model Training Notebook\")\n",
    "print(\"==================================\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"ğŸš€ GPU available: {len(gpus)} device(s)\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  {gpu}\")\nelse:\n",
    "    print(\"ğŸ’» Running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display configuration\n",
    "with open('../configs/training_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"ğŸ“‹ Training Configuration:\")\n",
    "print(f\"  Model Architecture: {config['model']['architecture']}\")\n",
    "print(f\"  Input Size: {config['model']['input_size']}\")\n",
    "print(f\"  Number of Classes: {config['model']['num_classes']}\")\n",
    "print(f\"  Epochs: {config['training']['epochs']}\")\n",
    "print(f\"  Batch Size: {config['training']['batch_size']}\")\n",
    "print(f\"  Learning Rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Optimizer: {config['training']['optimizer']}\")\n",
    "print(f\"  Data Augmentation: {config['data']['augmentation']['enabled']}\")\n",
    "\n",
    "print(f\"\\nğŸ·ï¸  Classes: {config['classes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check processed data availability\n",
    "processed_data_path = Path('../data/processed')\n",
    "\n",
    "if not processed_data_path.exists():\n",
    "    print(\"âŒ Processed data not found!\")\n",
    "    print(\"Please run the data preprocessing notebook first.\")\n",
    "    print(\"File: 02_data_preprocessing.ipynb\")\nelse:\n",
    "    print(\"âœ… Processed data found\")\n",
    "    \n",
    "    # Display data statistics\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        split_path = processed_data_path / split\n",
    "        if split_path.exists():\n",
    "            total_images = 0\n",
    "            print(f\"\\nğŸ“Š {split.capitalize()} Data:\")\n",
    "            \n",
    "            for class_dir in split_path.iterdir():\n",
    "                if class_dir.is_dir():\n",
    "                    class_count = len(list(class_dir.glob('*.jpg')))\n",
    "                    total_images += class_count\n",
    "                    print(f\"  {class_dir.name}: {class_count} images\")\n",
    "            \n",
    "            print(f\"  Total: {total_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer and setup data generators\n",
    "if processed_data_path.exists():\n",
    "    print(\"ğŸ”„ Initializing trainer and data generators...\")\n",
    "    \n",
    "    trainer = InfraOwlTrainer('../configs/training_config.yaml')\n",
    "    \n",
    "    print(\"âœ… Data generators created successfully!\")\n",
    "    print(f\"ğŸ“Š Training samples: {trainer.train_generator.samples}\")\n",
    "    print(f\"ğŸ“Š Validation samples: {trainer.val_generator.samples}\")\n",
    "    print(f\"ğŸ“Š Test samples: {trainer.test_generator.samples}\")\n",
    "    print(f\"ğŸ“Š Classes: {list(trainer.train_generator.class_indices.keys())}\")\nelse:\n",
    "    print(\"â­ï¸  Skipping trainer initialization (no processed data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and examine model architecture\n",
    "if processed_data_path.exists():\n",
    "    print(\"ğŸ—ï¸  Creating model architecture...\")\n",
    "    \n",
    "    model = trainer.create_model()\n",
    "    model = trainer.compile_model(model)\n",
    "    \n",
    "    print(\"\\nğŸ“‹ Model Summary:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Visualize model architecture\n",
    "    print(\"\\nğŸ“Š Model Structure:\")\n",
    "    print(f\"Total parameters: {model.count_params():,}\")\n",
    "    \n",
    "    # Count trainable vs non-trainable parameters\n",
    "    trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "    non_trainable_params = sum([tf.keras.backend.count_params(w) for w in model.non_trainable_weights])\n",
    "    \n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params:,}\")\n",
    "    \n",
    "    # Plot model architecture (if possible)\n",
    "    try:\n",
    "        tf.keras.utils.plot_model(\n",
    "            model, \n",
    "            to_file='../outputs/model_architecture.png',\n",
    "            show_shapes=True,\n",
    "            show_layer_names=True,\n",
    "            rankdir='TB',\n",
    "            dpi=150\n",
    "        )\n",
    "        print(\"ğŸ“ˆ Model architecture diagram saved to ../outputs/model_architecture.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not generate architecture diagram: {e}\")\nelse:\n",
    "    print(\"â­ï¸  Skipping model creation (no processed data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training data with augmentation\n",
    "if processed_data_path.exists() and config['data']['augmentation']['enabled']:\n",
    "    print(\"ğŸ”„ Visualizing data augmentation...\")\n",
    "    \n",
    "    # Get a batch of training data\n",
    "    sample_batch = next(trainer.train_generator)\n",
    "    sample_images, sample_labels = sample_batch\n",
    "    \n",
    "    # Display augmented samples\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    for i in range(min(8, len(sample_images))):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        \n",
    "        # Denormalize image for display\n",
    "        img = sample_images[i]\n",
    "        if img.max() <= 1.0:  # If normalized\n",
    "            img = (img * 255).astype('uint8')\n",
    "        \n",
    "        axes[row, col].imshow(img)\n",
    "        \n",
    "        # Get class name\n",
    "        class_idx = np.argmax(sample_labels[i])\n",
    "        class_name = list(trainer.train_generator.class_indices.keys())[class_idx]\n",
    "        \n",
    "        axes[row, col].set_title(f'{class_name}')\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.suptitle('Training Data with Augmentation')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Reset generator\n",
    "    trainer.train_generator.reset()\n",
    "elif processed_data_path.exists():\n",
    "    print(\"â„¹ï¸  Data augmentation is disabled\")\nelse:\n",
    "    print(\"â­ï¸  Skipping augmentation visualization (no processed data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Callbacks Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and display training callbacks\n",
    "if processed_data_path.exists():\n",
    "    print(\"âš™ï¸  Setting up training callbacks...\")\n",
    "    \n",
    "    callbacks = trainer.setup_callbacks()\n",
    "    \n",
    "    print(f\"ğŸ“‹ Configured Callbacks: {len(callbacks)}\")\n",
    "    for i, callback in enumerate(callbacks, 1):\n",
    "        callback_name = callback.__class__.__name__\n",
    "        print(f\"  {i}. {callback_name}\")\n",
    "        \n",
    "        # Show specific callback configurations\n",
    "        if callback_name == 'ModelCheckpoint':\n",
    "            print(f\"     Saving best model to: {callback.filepath}\")\n",
    "        elif callback_name == 'EarlyStopping':\n",
    "            print(f\"     Monitoring: {callback.monitor}, Patience: {callback.patience}\")\n",
    "        elif callback_name == 'ReduceLROnPlateau':\n",
    "            print(f\"     Reducing LR by factor {callback.factor} with patience {callback.patience}\")\n",
    "        elif callback_name == 'TensorBoard':\n",
    "            print(f\"     Logging to: {callback.log_dir}\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Monitor training with TensorBoard:\")\n",
    "    print(\"   tensorboard --logdir ../logs\")\nelse:\n",
    "    print(\"â­ï¸  Skipping callbacks setup (no processed data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "if processed_data_path.exists():\n",
    "    print(\"ğŸš€ Starting model training...\")\n",
    "    print(f\"Training for {config['training']['epochs']} epochs\")\n",
    "    print(\"This may take several minutes to hours depending on:\")\n",
    "    print(\"  â€¢ Dataset size\")\n",
    "    print(\"  â€¢ Model complexity\")\n",
    "    print(\"  â€¢ Hardware (CPU vs GPU)\")\n",
    "    print(\"\\nTraining progress will be displayed below...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        history = trainer.train_model(model, callbacks)\n",
    "        \n",
    "        print(\"\\nâœ… Training completed successfully!\")\n",
    "        \n",
    "        # Load best model from checkpoint\n",
    "        best_model_path = Path('../models/checkpoints/best_model.h5')\n",
    "        if best_model_path.exists():\n",
    "            model = keras.models.load_model(str(best_model_path))\n",
    "            print(\"âœ… Loaded best model from checkpoint\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Training failed: {e}\")\n",
    "        history = None\n",
    "        \nelse:\n",
    "    print(\"â­ï¸  Skipping training (no processed data)\")\n",
    "    history = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "if history is not None:\n",
    "    print(\"ğŸ“ˆ Visualizing training results...\")\n",
    "    \n",
    "    # Create comprehensive training plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Accuracy\n",
    "    axes[0, 0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[0, 0].set_title('Model Accuracy')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Loss\n",
    "    axes[0, 1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    axes[0, 1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[0, 1].set_title('Model Loss')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Learning Rate (if available)\n",
    "    if 'lr' in history.history:\n",
    "        axes[1, 0].plot(history.history['lr'], linewidth=2, color='orange')\n",
    "        axes[1, 0].set_title('Learning Rate Schedule')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'Learning Rate\\nNot Tracked', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].set_title('Learning Rate Schedule')\n",
    "    \n",
    "    # Plot 4: Training Summary\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    \n",
    "    summary_text = f\"\"\"Training Summary:\n",
    "    \nEpochs: {len(history.history['accuracy'])}\n",
    "    Final Train Accuracy: {final_train_acc:.3f}\n",
    "    Final Val Accuracy: {final_val_acc:.3f}\n",
    "    Best Val Accuracy: {best_val_acc:.3f}\n",
    "    \n",
    "    Model: {config['model']['architecture']}\n",
    "    Batch Size: {config['training']['batch_size']}\n",
    "    Learning Rate: {config['training']['learning_rate']}\"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.9, summary_text, transform=axes[1, 1].transAxes, \n",
    "                    fontsize=12, verticalalignment='top', fontfamily='monospace')\n",
    "    axes[1, 1].set_title('Training Summary')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/training_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"ğŸ“Š Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"ğŸ“ˆ Training plots saved to ../outputs/training_results.png\")\n",
    "    \nelse:\n",
    "    print(\"â­ï¸  Skipping training visualization (no training history)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "if history is not None and processed_data_path.exists():\n",
    "    print(\"ğŸ“Š Evaluating model on test data...\")\n",
    "    \n",
    "    try:\n",
    "        # Evaluate on test set\n",
    "        test_loss, test_accuracy = trainer.evaluate_model(model)\n",
    "        \n",
    "        print(f\"\\nğŸ¯ Final Test Results:\")\n",
    "        print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "        \n",
    "        # Generate predictions for confusion matrix\n",
    "        print(\"\\nğŸ”® Generating predictions...\")\n",
    "        predictions = model.predict(trainer.test_generator, verbose=1)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = trainer.test_generator.classes\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        from sklearn.metrics import confusion_matrix, classification_report\n",
    "        \n",
    "        cm = confusion_matrix(true_classes, predicted_classes)\n",
    "        class_names = list(trainer.test_generator.class_indices.keys())\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title('Confusion Matrix - Test Set')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../outputs/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Classification report\n",
    "        report = classification_report(true_classes, predicted_classes, \n",
    "                                     target_names=class_names)\n",
    "        print(\"\\nğŸ“‹ Classification Report:\")\n",
    "        print(report)\n",
    "        \n",
    "        # Save detailed results\n",
    "        with open('../outputs/test_results.txt', 'w') as f:\n",
    "            f.write(f\"InfraOwl Model Test Results\\n\")\n",
    "            f.write(f\"========================\\n\\n\")\n",
    "            f.write(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "            f.write(f\"Test Loss: {test_loss:.4f}\\n\\n\")\n",
    "            f.write(f\"Classification Report:\\n\")\n",
    "            f.write(report)\n",
    "        \n",
    "        print(\"ğŸ“„ Detailed results saved to ../outputs/test_results.txt\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Evaluation failed: {e}\")\n",
    "        \nelse:\n",
    "    print(\"â­ï¸  Skipping evaluation (no trained model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "if history is not None:\n",
    "    print(\"ğŸ’¾ Saving trained model...\")\n",
    "    \n",
    "    try:\n",
    "        trainer.save_model(model)\n",
    "        \n",
    "        # Verify saved model\n",
    "        saved_model_path = Path('../models/saved_models/infraowl_model.h5')\n",
    "        if saved_model_path.exists():\n",
    "            model_size = saved_model_path.stat().st_size / (1024 * 1024)  # MB\n",
    "            print(f\"âœ… Model saved successfully!\")\n",
    "            print(f\"ğŸ“ Location: {saved_model_path}\")\n",
    "            print(f\"ğŸ“ Size: {model_size:.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Model saving failed: {e}\")\n",
    "        \nelse:\n",
    "    print(\"â­ï¸  Skipping model saving (no trained model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TensorFlow Lite Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow Lite for mobile deployment\n",
    "if history is not None:\n",
    "    print(\"ğŸ“± Converting to TensorFlow Lite...\")\n",
    "    \n",
    "    try:\n",
    "        converter = TFLiteConverter('../configs/training_config.yaml')\n",
    "        converter.run_conversion()\n",
    "        \n",
    "        print(\"\\nâœ… TensorFlow Lite conversion completed!\")\n",
    "        \n",
    "        # Show TFLite models\n",
    "        tflite_dir = Path('../models/tflite_models')\n",
    "        if tflite_dir.exists():\n",
    "            print(\"\\nğŸ“± TensorFlow Lite Models:\")\n",
    "            for tflite_file in tflite_dir.glob('*.tflite'):\n",
    "                size_mb = tflite_file.stat().st_size / (1024 * 1024)\n",
    "                print(f\"  {tflite_file.name}: {size_mb:.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ TFLite conversion failed: {e}\")\n",
    "        print(\"You can manually convert later using:\")\n",
    "        print(\"python ../scripts/convert_to_tflite.py\")\n",
    "        \nelse:\n",
    "    print(\"â­ï¸  Skipping TFLite conversion (no trained model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"ğŸ¯ InfraOwl Training Summary\")\n",
    "print(\"==========================\")\n",
    "\n",
    "if history is not None:\n",
    "    print(\"âœ… Training completed successfully!\")\n",
    "    print(\"\\nğŸ“Š Results:\")\n",
    "    print(f\"  Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"  Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"  Best Validation Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    \n",
    "    if 'test_accuracy' in locals():\n",
    "        print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nğŸ“ Generated Files:\")\n",
    "    output_files = [\n",
    "        '../models/saved_models/infraowl_model.h5',\n",
    "        '../models/checkpoints/best_model.h5',\n",
    "        '../outputs/training_results.png',\n",
    "        '../outputs/training_history.png',\n",
    "        '../outputs/model_summary.txt'\n",
    "    ]\n",
    "    \n",
    "    for file_path in output_files:\n",
    "        if Path(file_path).exists():\n",
    "            print(f\"  âœ… {file_path}\")\n",
    "        else:\n",
    "            print(f\"  âŒ {file_path}\")\n",
    "    \n",
    "    print(\"\\nğŸš€ Next Steps:\")\n",
    "    print(\"1. ğŸ“Š Detailed Evaluation:\")\n",
    "    print(\"   python ../scripts/evaluate_model.py\")\n",
    "    print(\"\\n2. ğŸ“± Deploy to Flutter App:\")\n",
    "    print(\"   Copy the TFLite model to ../assets/\")\n",
    "    print(\"   Update the labels.txt file if needed\")\n",
    "    print(\"\\n3. ğŸ”„ Improve Model (if needed):\")\n",
    "    print(\"   â€¢ Collect more training data\")\n",
    "    print(\"   â€¢ Adjust hyperparameters\")\n",
    "    print(\"   â€¢ Try different architectures\")\n",
    "    print(\"   â€¢ Increase training epochs\")\n",
    "    \nelse:\n",
    "    print(\"âŒ Training was not completed\")\n",
    "    print(\"\\nTo start training:\")\n",
    "    print(\"1. Ensure processed data exists (run 02_data_preprocessing.ipynb)\")\n",
    "    print(\"2. Re-run this notebook or use: python ../scripts/train_model.py\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Tips:\")\n",
    "print(\"â€¢ Monitor training with TensorBoard: tensorboard --logdir ../logs\")\n",
    "print(\"â€¢ Adjust configuration in ../configs/training_config.yaml\")\n",
    "print(\"â€¢ Use GPU for faster training if available\")\n",
    "print(\"â€¢ Regularly backup your trained models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}